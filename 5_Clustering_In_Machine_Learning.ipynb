{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. K-Means Clustering**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **K-Means** is an **unsupervised learning algorithm** used for **clustering**.\n",
        "* It groups similar data points into **K clusters**, where each cluster is represented by its **centroid** (mean of points).\n",
        "* Goal: Minimize the distance between data points and their assigned cluster centroid.\n",
        "\n",
        "üìå Example:\n",
        "Segmenting customers into groups based on their **purchasing behavior**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How K-Means Works (Step by Step)**\n",
        "\n",
        "1. **Choose number of clusters (K).**\n",
        "\n",
        "   * Example: If K=3, we want to split data into 3 groups.\n",
        "\n",
        "2. **Initialize centroids.**\n",
        "\n",
        "   * Randomly select K points from the dataset as initial centroids.\n",
        "\n",
        "3. **Assign each point to the nearest centroid.**\n",
        "\n",
        "   * Compute distance (e.g., Euclidean) from each point to centroids.\n",
        "   * Assign each point to the cluster with the closest centroid.\n",
        "\n",
        "4. **Update centroids.**\n",
        "\n",
        "   * For each cluster, compute the **mean of all assigned points**.\n",
        "   * This mean becomes the new centroid.\n",
        "\n",
        "5. **Repeat steps 3 & 4** until convergence.\n",
        "\n",
        "   * Stop when centroids no longer move significantly (or max iterations reached).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mathematical Formulation**\n",
        "\n",
        "Objective: Minimize the **within-cluster sum of squares (WCSS)**\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^K \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C_i$ = set of points in cluster $i$\n",
        "* $\\mu_i$ = centroid of cluster $i$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Choosing K (Number of Clusters)**\n",
        "\n",
        "### üîπ Elbow Method\n",
        "\n",
        "* Plot **WCSS vs K**.\n",
        "* Look for the \"elbow\" point where adding more clusters doesn‚Äôt improve much.\n",
        "\n",
        "### üîπ Silhouette Score\n",
        "\n",
        "* Measures how well points fit within their cluster compared to others.\n",
        "* Ranges from **-1 to 1** (higher is better).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* Simple & easy to implement.\n",
        "* Scales well for large datasets.\n",
        "* Works well when clusters are spherical & well-separated.\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Need to predefine **K**.\n",
        "* Sensitive to initialization (different results on different runs).\n",
        "* Sensitive to outliers (can distort centroids).\n",
        "* Works poorly with non-spherical clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Assumptions**\n",
        "\n",
        "* Clusters are convex, isotropic (roughly spherical).\n",
        "* Data points are closer to their cluster centroid than others.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Variants**\n",
        "\n",
        "* **K-Means++:** Smarter centroid initialization ‚Üí improves stability.\n",
        "* **Mini-Batch K-Means:** Uses batches for faster training on large datasets.\n",
        "* **Fuzzy C-Means:** Allows soft assignments (probability of belonging to clusters).\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Real-Life Applications**\n",
        "\n",
        "* **Customer Segmentation:** Grouping customers by spending habits.\n",
        "* **Market Basket Analysis:** Grouping similar products.\n",
        "* **Image Compression:** Reducing colors by clustering similar pixels.\n",
        "* **Anomaly Detection:** Outliers that don‚Äôt belong to any cluster.\n",
        "* **Document Clustering:** Grouping similar news/articles.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Visualization**\n",
        "\n",
        "```\n",
        "Step 1: Initialize centroids (random points)\n",
        "Step 2: Assign points to nearest centroid\n",
        "Step 3: Recompute centroids\n",
        "Step 4: Repeat until convergence\n",
        "```\n",
        "\n",
        "Example (K=3):\n",
        "\n",
        "```\n",
        " ‚óè ‚óè ‚óè ‚óã ‚óã ‚óã ‚ñ≤ ‚ñ≤ ‚ñ≤\n",
        "     ‚Üì Clustering ‚Üì\n",
        " Cluster 1   Cluster 2   Cluster 3\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* K-Means = **distance-based clustering**.\n",
        "* Objective = minimize within-cluster variance.\n",
        "* Needs good choice of **K** and **initialization**.\n",
        "* Works best on spherical, well-separated clusters.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **2. Hierarchical Clustering**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Hierarchical Clustering** builds a **hierarchy of clusters**.\n",
        "* Instead of partitioning data into K clusters directly (like K-Means), it creates a **tree-like structure (dendrogram)**.\n",
        "* You can \"cut\" the dendrogram at a certain level to decide how many clusters to form.\n",
        "\n",
        "üìå Example:\n",
        "Grouping species based on DNA similarity ‚Äî you see a hierarchy where closer species join earlier, and distant species join later.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Types of Hierarchical Clustering**\n",
        "\n",
        "### üîπ **Agglomerative (Bottom-Up)**\n",
        "\n",
        "* Start with each point as its own cluster.\n",
        "* Iteratively merge the closest clusters.\n",
        "* Stop when all points are in one big cluster.\n",
        "* Most commonly used.\n",
        "\n",
        "### üîπ **Divisive (Top-Down)**\n",
        "\n",
        "* Start with one big cluster (all points).\n",
        "* Iteratively split into smaller clusters.\n",
        "* Less common (more computationally expensive).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Distance Metrics (to measure closeness between points)**\n",
        "\n",
        "* **Euclidean Distance:**\n",
        "\n",
        "  $$\n",
        "  d(x,y) = \\sqrt{\\sum (x_i - y_i)^2}\n",
        "  $$\n",
        "* **Manhattan Distance:**\n",
        "\n",
        "  $$\n",
        "  d(x,y) = \\sum |x_i - y_i|\n",
        "  $$\n",
        "* **Cosine Distance:** Based on angle between vectors (good for text).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Linkage Criteria (to measure closeness between clusters)**\n",
        "\n",
        "When merging clusters, we need a rule to decide \"closest\".\n",
        "\n",
        "* **Single Linkage:**\n",
        "  Minimum distance between any two points in clusters.\n",
        "  (Tends to form \"chains\").\n",
        "\n",
        "* **Complete Linkage:**\n",
        "  Maximum distance between any two points.\n",
        "  (Tends to form compact clusters).\n",
        "\n",
        "* **Average Linkage:**\n",
        "  Average distance between all pairs.\n",
        "  (Balance between single & complete).\n",
        "\n",
        "* **Ward‚Äôs Method:**\n",
        "  Merge clusters that result in the smallest increase in total variance.\n",
        "  (Very popular for compact clusters).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Workflow (Agglomerative Clustering)**\n",
        "\n",
        "1. Treat each point as its own cluster.\n",
        "2. Compute pairwise distances between all clusters.\n",
        "3. Merge the two closest clusters.\n",
        "4. Recompute distances between clusters.\n",
        "5. Repeat until one cluster remains.\n",
        "6. Use **dendrogram** to decide final number of clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Dendrogram**\n",
        "\n",
        "A dendrogram is a **tree diagram** that shows how clusters are merged step by step.\n",
        "\n",
        "* Height of the join = distance between clusters.\n",
        "* Cutting horizontally across the dendrogram = choosing number of clusters.\n",
        "\n",
        "üìå Example:\n",
        "If you cut at a certain height and see 3 vertical lines intersecting ‚Üí you have 3 clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* No need to pre-specify number of clusters (can decide using dendrogram).\n",
        "* Works for different cluster shapes and sizes.\n",
        "* Provides hierarchy (multi-level clustering).\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Computationally expensive for large datasets (**O(n¬≤ log n)**).\n",
        "* Once merged/split, decisions can‚Äôt be undone.\n",
        "* Sensitive to noise and outliers.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Real-Life Applications**\n",
        "\n",
        "* **Biology:** Phylogenetic trees, DNA sequence grouping.\n",
        "* **Marketing:** Customer segmentation.\n",
        "* **Text Mining:** Document clustering.\n",
        "* **Image Processing:** Grouping pixels for segmentation.\n",
        "* **Sociology:** Grouping people based on survey responses.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Visualization**\n",
        "\n",
        "```\n",
        "Data Points ‚Üí Compute Distances ‚Üí Merge Closest Clusters ‚Üí Dendrogram\n",
        "\n",
        "Example (dendrogram):\n",
        "\n",
        "|          ______ Cluster A\n",
        "|      ___|\n",
        "|     |   |______ Cluster B\n",
        "|  ___|\n",
        "| |   |_________ Cluster C\n",
        "| |\n",
        "| |_____________ Cluster D\n",
        "|\n",
        "+--------------------------------\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* Hierarchical Clustering = builds a **tree of clusters**.\n",
        "* Two main types: **Agglomerative (bottom-up)**, **Divisive (top-down)**.\n",
        "* Uses **distance metrics** + **linkage criteria** to merge/split.\n",
        "* Visualized using a **dendrogram**.\n",
        "* Works well for small/medium datasets, not great for very large ones.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **3. DBSCAN (Density-Based Clustering)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **DBSCAN** is a **density-based clustering algorithm**.\n",
        "* Instead of partitioning data into K groups (like K-Means), it identifies **dense regions** of points as clusters and labels low-density points as **noise/outliers**.\n",
        "* It is great for discovering **arbitrarily shaped clusters** (not just spherical ones).\n",
        "\n",
        "üìå Example:\n",
        "Separating urban regions (dense data) from rural regions (sparse data) in geographical mapping.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Concepts**\n",
        "\n",
        "DBSCAN relies on two parameters:\n",
        "\n",
        "1. **Œµ (epsilon):** Radius defining the neighborhood around a point.\n",
        "2. **MinPts (minimum points):** Minimum number of points required to form a dense region.\n",
        "\n",
        "Based on these, points are classified as:\n",
        "\n",
        "* **Core Point:** Has at least MinPts within Œµ.\n",
        "* **Border Point:** Lies within Œµ of a core point but has fewer than MinPts.\n",
        "* **Noise (Outlier):** Not a core point and not within Œµ of any core point.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. DBSCAN Algorithm (Step by Step)**\n",
        "\n",
        "1. Pick an unvisited point.\n",
        "2. If it has at least **MinPts** neighbors within **Œµ** ‚Üí mark as a **core point** and form a new cluster.\n",
        "3. Expand cluster: Add all reachable points within Œµ.\n",
        "4. If the point is not a core point ‚Üí mark as **noise** (can later become border if part of a cluster).\n",
        "5. Repeat until all points are visited.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Mathematical Definition**\n",
        "\n",
        "Two key definitions:\n",
        "\n",
        "* **Directly Density-Reachable:** A point $p$ is directly density-reachable from $q$ if $p$ lies within Œµ of $q$ and $q$ is a core point.\n",
        "* **Density-Connected:** Two points $p$ and $q$ are density-connected if there exists a chain of points between them where each pair is directly density-reachable.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* No need to specify number of clusters (unlike K-Means).\n",
        "* Can find **arbitrarily shaped clusters**.\n",
        "* Identifies **outliers** naturally.\n",
        "* Works well with spatial/geographic data.\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Choice of **Œµ** and **MinPts** is tricky.\n",
        "* Struggles with clusters of **varying densities**.\n",
        "* Computationally expensive for very high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Choosing Parameters**\n",
        "\n",
        "* **MinPts:** Typically ‚â• dimension + 1 (e.g., for 2D data, MinPts ‚â• 3‚Äì4).\n",
        "* **Œµ:** Use **k-distance plot** ‚Üí plot distances to the k-th nearest neighbor and look for the ‚Äúelbow.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Geography:** Identifying urban vs rural areas from satellite data.\n",
        "* **Fraud Detection:** Outliers in transaction data.\n",
        "* **Astronomy:** Finding star clusters in noisy space data.\n",
        "* **Biology:** Clustering gene expression data.\n",
        "* **Market Analysis:** Finding customer groups with irregular purchase behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization Example**\n",
        "\n",
        "```\n",
        "Dense regions ‚Üí Clusters\n",
        "Sparse points ‚Üí Noise\n",
        "\n",
        "‚óè ‚óè ‚óè ‚óè ‚óã   ‚óã         ‚ñ≤ ‚ñ≤ ‚ñ≤ ‚ñ≤ ‚ñ≤\n",
        "‚óè ‚óè ‚óè ‚óè ‚óã             ‚ñ≤ ‚ñ≤ ‚ñ≤ ‚ñ≤\n",
        "‚óè ‚óè ‚óè ‚óã               ‚óã (noise)\n",
        "\n",
        "Clusters = (‚óè group, ‚ñ≤ group), Noise = ‚óã\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Comparison with K-Means & Hierarchical**\n",
        "\n",
        "| Feature             | K-Means   | Hierarchical             | DBSCAN               |\n",
        "| ------------------- | --------- | ------------------------ | -------------------- |\n",
        "| Need K?             | Yes       | No (use dendrogram)      | No (density decides) |\n",
        "| Cluster Shape       | Spherical | Any (depends on linkage) | Arbitrary (best)     |\n",
        "| Outlier Handling    | Poor      | Poor                     | Excellent            |\n",
        "| Large Data Handling | Good      | Poor                     | Moderate             |\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* DBSCAN = **density-based clustering**.\n",
        "* Identifies **arbitrarily shaped clusters** and **outliers**.\n",
        "* Needs careful selection of **Œµ** and **MinPts**.\n",
        "* Works best when clusters have clear density differences.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "4sH0r37lsB2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHSnBwDYoz3h"
      },
      "outputs": [],
      "source": []
    }
  ]
}