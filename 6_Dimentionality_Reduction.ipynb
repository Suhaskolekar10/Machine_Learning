{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Principal Component Analysis (PCA)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **PCA** is a **dimensionality reduction technique** used to transform high-dimensional data into fewer dimensions while preserving as much **variance** (information) as possible.\n",
        "* It creates **new features (principal components)** that are linear combinations of the original features.\n",
        "* Useful when:\n",
        "\n",
        "  * Data has many correlated features.\n",
        "  * You want to visualize high-dimensional data in 2D/3D.\n",
        "  * Reduce noise and improve ML model performance.\n",
        "\n",
        "ğŸ“Œ Example:\n",
        "Reducing 100 stock market indicators to 5 key â€œprincipal componentsâ€ that explain most of the variation.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Idea**\n",
        "\n",
        "* Find new axes (**principal components**) such that:\n",
        "\n",
        "  * The **first component** explains the maximum variance.\n",
        "  * The **second component** explains the maximum remaining variance (orthogonal to first).\n",
        "  * Continue until all components are extracted.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. PCA Workflow (Step by Step)**\n",
        "\n",
        "1. **Standardize the data**\n",
        "   (important because PCA is affected by feature scale).\n",
        "\n",
        "2. **Compute covariance matrix**\n",
        "\n",
        "   $$\n",
        "   \\Sigma = \\frac{1}{n} (X - \\bar{X})^T (X - \\bar{X})\n",
        "   $$\n",
        "\n",
        "3. **Compute eigenvalues & eigenvectors** of covariance matrix.\n",
        "\n",
        "   * Eigenvectors â†’ Principal Components (directions of maximum variance).\n",
        "   * Eigenvalues â†’ Variance explained by each component.\n",
        "\n",
        "4. **Sort eigenvectors by eigenvalues** (descending order).\n",
        "\n",
        "5. **Choose top k components** that explain most of the variance.\n",
        "\n",
        "6. **Transform data** into new subspace.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Mathematical Intuition**\n",
        "\n",
        "We want to maximize variance along new axis:\n",
        "\n",
        "$$\n",
        "\\text{Var}(z) = w^T \\Sigma w\n",
        "$$\n",
        "\n",
        "Subject to constraint:\n",
        "\n",
        "$$\n",
        "||w|| = 1\n",
        "$$\n",
        "\n",
        "This is solved by eigen-decomposition â†’ eigenvector with largest eigenvalue gives **first principal component**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Explained Variance**\n",
        "\n",
        "* Eigenvalues indicate how much variance each principal component captures.\n",
        "* **Explained Variance Ratio:**\n",
        "\n",
        "$$\n",
        "\\text{EVR}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\n",
        "$$\n",
        "\n",
        "ğŸ“Œ Rule: Keep enough components to capture **~90-95%** of variance.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pros & Cons**\n",
        "\n",
        "### âœ… Pros\n",
        "\n",
        "* Reduces dimensionality â†’ faster training.\n",
        "* Removes multicollinearity.\n",
        "* Visualization in 2D/3D possible.\n",
        "* Can denoise data.\n",
        "\n",
        "### âŒ Cons\n",
        "\n",
        "* Components are **linear combinations** (less interpretable).\n",
        "* Sensitive to scaling of features.\n",
        "* Assumes linear relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Finance:** Reduce correlated indicators into few principal indexes.\n",
        "* **Image Compression:** Reduce pixel space while retaining key patterns.\n",
        "* **Genomics:** Reduce thousands of gene expressions into few key dimensions.\n",
        "* **Marketing:** Customer segmentation with reduced features.\n",
        "* **Speech Recognition:** Reduce acoustic features before classification.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization (Conceptual)**\n",
        "\n",
        "Original space (X, Y features):\n",
        "\n",
        "```\n",
        " â— â— â— â— â— â—\n",
        "      â†˜  New Axis (PC1: Max variance)\n",
        "       â†˜\n",
        "        â†˜  (PC2: Orthogonal to PC1)\n",
        "```\n",
        "\n",
        "After PCA â†’ rotate axes â†’ project data into fewer dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Example**\n",
        "\n",
        "Dataset: Students with features [Math, Science, English Scores].\n",
        "\n",
        "* PCA may reduce this into 2 components:\n",
        "\n",
        "  * PC1 = overall academic strength.\n",
        "  * PC2 = preference for math/science vs language.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* PCA = transforms correlated features â†’ uncorrelated principal components.\n",
        "* Helps in **dimensionality reduction, noise removal, visualization**.\n",
        "* Always standardize data before applying PCA.\n",
        "* Decide number of PCs using **explained variance (scree plot / elbow method)**.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **2. t-SNE (t-distributed Stochastic Neighbor Embedding)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **t-SNE** is a **non-linear dimensionality reduction technique** used primarily for **visualization** of high-dimensional data in **2D or 3D**.\n",
        "* Unlike PCA (linear), t-SNE preserves **local structure**, meaning similar points in high dimensions stay close in low dimensions.\n",
        "* Commonly used for:\n",
        "\n",
        "  * Image embeddings\n",
        "  * Word embeddings (NLP)\n",
        "  * Clustering visualization\n",
        "\n",
        "ğŸ“Œ Example:\n",
        "Visualizing handwritten digits (MNIST dataset) in 2D so that digits 0â€“9 form separate clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Idea**\n",
        "\n",
        "1. Compute **pairwise similarities** between points in high-dimensional space (using Gaussian distribution).\n",
        "2. Map points to low-dimensional space such that **similar points stay close**, and **dissimilar points stay apart**.\n",
        "3. Minimize **Kullback-Leibler (KL) divergence** between high-dimensional and low-dimensional distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. How t-SNE Works (Conceptually)**\n",
        "\n",
        "1. **Compute pairwise probabilities** $p_{ij}$ in high-dimensional space:\n",
        "\n",
        "   $$\n",
        "   p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
        "   $$\n",
        "\n",
        "   * Symmetrize: $p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}$\n",
        "\n",
        "2. **Map points to low-dimensional space** (y_i).\n",
        "\n",
        "3. Compute **similarity $q_{ij}$** in low-dimensional space using **Student-t distribution**:\n",
        "\n",
        "   $$\n",
        "   q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
        "   $$\n",
        "\n",
        "4. **Minimize KL divergence** between high-dimensional $p_{ij}$ and low-dimensional $q_{ij}$:\n",
        "\n",
        "   $$\n",
        "   KL(P||Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Key Features**\n",
        "\n",
        "* Preserves **local neighborhood structure**.\n",
        "* Can reveal **clusters** in data visually.\n",
        "* **Non-linear mapping**, unlike PCA.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### âœ… Pros\n",
        "\n",
        "* Excellent for **visualizing high-dimensional data**.\n",
        "* Reveals hidden **clusters or patterns**.\n",
        "* Preserves local similarities better than linear methods.\n",
        "\n",
        "### âŒ Cons\n",
        "\n",
        "* Computationally expensive for very large datasets.\n",
        "* Does not scale well beyond tens of thousands of points (though **Barnes-Hut t-SNE** improves speed).\n",
        "* Non-deterministic results (may vary across runs).\n",
        "* Low-dimensional distances are **not always meaningful globally**.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Hyperparameters**\n",
        "\n",
        "* **Perplexity:** Roughly controls the number of neighbors (5â€“50 common).\n",
        "* **Learning Rate:** Affects convergence (typical 200â€“1000).\n",
        "* **Number of Iterations:** More iterations â†’ better embedding.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **MNIST / Image Datasets:** Visualize digits or image embeddings.\n",
        "* **NLP:** Visualize word embeddings (Word2Vec, GloVe).\n",
        "* **Bioinformatics:** Gene expression data visualization.\n",
        "* **Clustering:** Visual validation of clusters.\n",
        "* **Anomaly Detection:** Visualize outliers in high-dimensional space.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization (Conceptual)**\n",
        "\n",
        "```\n",
        "High-dimensional points â†’ t-SNE â†’ 2D map\n",
        "\n",
        "High-D Space:    â— â— â— â— â—   (similar points close)\n",
        "Low-D Map:       â—‹ â—‹ â—‹ â—‹ â—‹   (similar points stay close, clusters appear)\n",
        "```\n",
        "\n",
        "Example: MNIST digits\n",
        "\n",
        "```\n",
        "Cluster 0  Cluster 1   Cluster 2\n",
        "â—â—â—       â—â—â—        â—â—â—\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Key Takeaways**\n",
        "\n",
        "* t-SNE = **non-linear, local-preserving dimensionality reduction**.\n",
        "* Best for **visualization**, not for direct ML modeling.\n",
        "* Reveals hidden clusters in high-dimensional datasets.\n",
        "* Sensitive to **perplexity and initialization**, so tuning is often needed.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "VHFmmVAO71vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Association Rule Mining**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* Association Rule Mining finds **if-then patterns** in data.\n",
        "* Itâ€™s most famous in **Market Basket Analysis** (e.g., â€œPeople who buy bread also buy butterâ€).\n",
        "* Works on **transactional datasets** (like shopping carts, clickstreams, medical records).\n",
        "\n",
        "ğŸ“Œ Example:\n",
        "\n",
        "* Rule: {Milk, Bread} â†’ {Butter}\n",
        "* Meaning: Customers who buy milk and bread are also likely to buy butter.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Terms**\n",
        "\n",
        "Letâ€™s assume a dataset of transactions (shopping baskets).\n",
        "\n",
        "* **Itemset:** A collection of items (e.g., {milk, bread}).\n",
        "\n",
        "* **Support:** Frequency of itemset in dataset.\n",
        "  [\n",
        "  Support(A) = \\frac{\\text{Number of transactions containing A}}{\\text{Total transactions}}\n",
        "  ]\n",
        "\n",
        "* **Confidence:** Strength of rule (probability of B given A).\n",
        "  [\n",
        "  Confidence(A \\rightarrow B) = \\frac{Support(A \\cup B)}{Support(A)}\n",
        "  ]\n",
        "\n",
        "* **Lift:** How much more likely A and B occur together compared to random chance.\n",
        "  [\n",
        "  Lift(A \\rightarrow B) = \\frac{Confidence(A \\rightarrow B)}{Support(B)}\n",
        "  ]\n",
        "\n",
        "  * Lift > 1 â†’ Positive association.\n",
        "  * Lift < 1 â†’ Negative association.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Example**\n",
        "\n",
        "Dataset:\n",
        "\n",
        "* T1: {Milk, Bread, Butter}\n",
        "* T2: {Milk, Bread}\n",
        "* T3: {Bread, Butter}\n",
        "* T4: {Milk, Butter}\n",
        "* T5: {Milk, Bread, Butter}\n",
        "\n",
        "Rule: **{Milk, Bread} â†’ {Butter}**\n",
        "\n",
        "* Support = 3/5 = 0.6\n",
        "* Confidence = 3/4 = 0.75\n",
        "* Lift = 0.75 / 0.6 = 1.25 (positive correlation).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Algorithms for Association Rule Mining**\n",
        "\n",
        "### ğŸ”¹ **Apriori Algorithm**\n",
        "\n",
        "* Iteratively generates frequent itemsets using support threshold.\n",
        "* Uses **downward closure property**: If an itemset is frequent, all its subsets must also be frequent.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Generate candidate itemsets.\n",
        "2. Prune based on minimum support.\n",
        "3. Generate rules with minimum confidence.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¹ **FP-Growth (Frequent Pattern Growth)**\n",
        "\n",
        "* Faster than Apriori (avoids candidate generation).\n",
        "* Builds a **prefix tree (FP-tree)** of itemsets.\n",
        "* Extracts frequent itemsets directly.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### âœ… Pros\n",
        "\n",
        "* Simple and intuitive (especially Apriori).\n",
        "* Helps in **recommendation systems**.\n",
        "* Useful for **business insights**.\n",
        "\n",
        "### âŒ Cons\n",
        "\n",
        "* Can generate **too many rules** (difficult to interpret).\n",
        "* Computationally expensive for very large datasets.\n",
        "* Focuses on correlation, not causation.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Real-Life Applications**\n",
        "\n",
        "* **Retail (Market Basket Analysis):** Find product bundles (e.g., chips + soda).\n",
        "* **E-commerce:** Cross-selling & upselling recommendations.\n",
        "* **Healthcare:** Discover symptomâ€“disease associations.\n",
        "* **Web Usage Mining:** Identify frequently co-visited pages.\n",
        "* **Fraud Detection:** Detect unusual item combinations.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Visualization**\n",
        "\n",
        "```\n",
        "{Milk, Bread} â†’ {Butter}\n",
        "Support = 60%\n",
        "Confidence = 75%\n",
        "Lift = 1.25\n",
        "```\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "* **Support:** How popular the rule is.\n",
        "* **Confidence:** How reliable the rule is.\n",
        "* **Lift:** How strong the association is compared to chance.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Key Takeaways**\n",
        "\n",
        "* Association Rule Mining finds **hidden relationships** in data.\n",
        "* Metrics: **Support, Confidence, Lift**.\n",
        "* Algorithms: **Apriori** (classic, slow) & **FP-Growth** (fast, scalable).\n",
        "* Widely used in **retail, healthcare, fraud detection, recommendation systems**.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "mVKkOfyPOoCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Anomaly Detection**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Anomalies (Outliers)** are data points that **donâ€™t follow the normal pattern** of the dataset.\n",
        "* Goal: Identify unusual observations that may indicate **errors, fraud, attacks, or rare events**.\n",
        "\n",
        "ğŸ“Œ Examples:\n",
        "\n",
        "* Unusually high credit card transaction â†’ Fraud.\n",
        "* Sudden spike in server traffic â†’ Cyberattack.\n",
        "* Abnormal medical test values â†’ Disease indication.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Types of Anomalies**\n",
        "\n",
        "1. **Point Anomaly:** A single data point is unusual.\n",
        "\n",
        "   * Example: A \\$10,000 withdrawal in a dataset where typical withdrawals are \\$100â€“$500.\n",
        "\n",
        "2. **Contextual Anomaly:** An observation is unusual in a specific context.\n",
        "\n",
        "   * Example: A high temperature of 35Â°C is normal in summer but abnormal in winter.\n",
        "\n",
        "3. **Collective Anomaly:** A group of data points together is abnormal.\n",
        "\n",
        "   * Example: Multiple failed login attempts in sequence â†’ Intrusion.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Approaches to Anomaly Detection**\n",
        "\n",
        "### ğŸ”¹ **Statistical Methods**\n",
        "\n",
        "* Assume data follows a distribution (e.g., Gaussian).\n",
        "* Flag points far from mean (using **z-scores** or thresholds).\n",
        "\n",
        "### ğŸ”¹ **Distance-Based Methods**\n",
        "\n",
        "* Points far from others are anomalies.\n",
        "* Example: **k-Nearest Neighbors (kNN)** anomaly detection.\n",
        "\n",
        "### ğŸ”¹ **Density-Based Methods**\n",
        "\n",
        "* Points in low-density regions are anomalies.\n",
        "* Example: **DBSCAN**, **Local Outlier Factor (LOF)**.\n",
        "\n",
        "### ğŸ”¹ **Clustering-Based**\n",
        "\n",
        "* Normal data forms clusters; anomalies donâ€™t belong to any cluster.\n",
        "* Example: K-Means anomaly detection.\n",
        "\n",
        "### ğŸ”¹ **Machine Learning Methods**\n",
        "\n",
        "* **One-Class SVM:** Learns a boundary around normal data, flags points outside.\n",
        "* **Isolation Forest:** Randomly splits data; anomalies are easier to isolate.\n",
        "* **Autoencoders (Deep Learning):** Reconstruct input data; anomalies have high reconstruction error.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Mathematical Intuition**\n",
        "\n",
        "### Z-Score Method (Statistical)\n",
        "\n",
        "[\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "]\n",
        "\n",
        "* If |z| > threshold (e.g., 3), point is anomaly.\n",
        "\n",
        "### Local Outlier Factor (LOF)\n",
        "\n",
        "* Ratio of density of a point to densities of neighbors.\n",
        "* LOF > 1 â†’ anomaly.\n",
        "\n",
        "### Isolation Forest\n",
        "\n",
        "* Randomly split features into trees.\n",
        "* Outliers â†’ isolated quickly (shorter path length).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### âœ… Pros\n",
        "\n",
        "* Works across finance, security, healthcare, IoT.\n",
        "* Can detect both known & unknown anomalies.\n",
        "* Many algorithm choices (statistical â†’ deep learning).\n",
        "\n",
        "### âŒ Cons\n",
        "\n",
        "* Often **domain-specific thresholds** needed.\n",
        "* High false positives (normal unusual data flagged as anomaly).\n",
        "* Some methods donâ€™t scale well with very high dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Real-Life Applications**\n",
        "\n",
        "* **Fraud Detection:** Credit card, insurance, tax fraud.\n",
        "* **Cybersecurity:** Intrusion detection, malware detection.\n",
        "* **Healthcare:** Detect rare diseases via abnormal test results.\n",
        "* **Manufacturing (IoT):** Machine failure prediction.\n",
        "* **Finance:** Stock market irregularities.\n",
        "* **Climate Science:** Detect abnormal weather events.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Visualization (Conceptual)**\n",
        "\n",
        "Normal data (clustered) vs anomalies:\n",
        "\n",
        "```\n",
        "â— â— â— â— â— â— â— â—   â—‹\n",
        "â— â— â— â— â— â— â— â—\n",
        "â— â— â— â— â—     â—‹\n",
        "```\n",
        "\n",
        "* â— = Normal points\n",
        "* â—‹ = Anomalies (outliers)\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Key Takeaways**\n",
        "\n",
        "* Anomalies = **data points deviating from normal patterns**.\n",
        "* Types: **Point, Contextual, Collective**.\n",
        "* Methods: Statistical, distance, density, clustering, ML.\n",
        "* Powerful applications in **fraud detection, cybersecurity, healthcare, IoT**.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "NUGmL_dAWQYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejHy69_w7woe"
      },
      "outputs": [],
      "source": []
    }
  ]
}