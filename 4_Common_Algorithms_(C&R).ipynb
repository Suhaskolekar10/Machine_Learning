{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **1. Decision Tree Algorithm**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* A **Decision Tree** is a supervised ML algorithm used for both **classification** and **regression** tasks.\n",
        "* It works by **splitting the dataset** into smaller subsets based on feature values, forming a **tree-like structure**.\n",
        "* Each **internal node** = a test on a feature.\n",
        "* Each **branch** = outcome of the test.\n",
        "* Each **leaf node** = final prediction (class label or value).\n",
        "\n",
        "📌 **Example (Classification):**\n",
        "\n",
        "* Predict whether a person buys a computer based on **Age** and **Income**.\n",
        "\n",
        "```\n",
        "                [Age <= 30?]\n",
        "                  /      \\\n",
        "             Yes /        \\ No\n",
        "                /          \\\n",
        "         [Income?]        Buy = Yes\n",
        "        /       \\\n",
        "  High /         \\ Low\n",
        "     /             \\\n",
        "Buy=No            Buy=Yes\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "1. **Select the best feature** to split the dataset (based on a splitting criterion like Gini, Entropy, or Variance Reduction).\n",
        "2. **Split the dataset** into subsets.\n",
        "3. Repeat recursively on each subset (recursive partitioning).\n",
        "4. Stop when:\n",
        "\n",
        "   * All samples belong to one class, or\n",
        "   * No further information gain possible, or\n",
        "   * Tree reaches max depth.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Splitting Criteria**\n",
        "\n",
        "### **a) For Classification Trees**\n",
        "\n",
        "1. **Entropy & Information Gain (ID3 algorithm)**\n",
        "\n",
        "   * **Entropy (measure of impurity):**\n",
        "\n",
        "   $$\n",
        "   Entropy(S) = - \\sum_{i=1}^c p_i \\log_2 p_i\n",
        "   $$\n",
        "\n",
        "   where $p_i$ = proportion of class $i$.\n",
        "\n",
        "   * **Information Gain (IG):**\n",
        "\n",
        "   $$\n",
        "   IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n",
        "   $$\n",
        "\n",
        "   Higher IG → better feature to split.\n",
        "\n",
        "2. **Gini Impurity (CART algorithm)**\n",
        "\n",
        "   * **Formula:**\n",
        "\n",
        "   $$\n",
        "   Gini(S) = 1 - \\sum_{i=1}^c p_i^2\n",
        "   $$\n",
        "\n",
        "   Lower Gini → purer node.\n",
        "\n",
        "---\n",
        "\n",
        "### **b) For Regression Trees**\n",
        "\n",
        "* Use **Variance Reduction** or **Mean Squared Error (MSE)**.\n",
        "\n",
        "$$\n",
        "Var(S) = \\frac{1}{|S|} \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
        "$$\n",
        "\n",
        "The split is chosen to minimize variance of child nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example (Classification Tree)**\n",
        "\n",
        "Suppose dataset: Predict “Play Tennis” based on “Weather.”\n",
        "\n",
        "* If **Weather = Sunny**, check **Humidity**.\n",
        "* If **Weather = Overcast**, always **Play = Yes**.\n",
        "* If **Weather = Rainy**, check **Windy**.\n",
        "\n",
        "This creates a simple decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Assumptions**\n",
        "\n",
        "* Features are sufficient to describe target outcome.\n",
        "* Data is split recursively until pure subsets are formed.\n",
        "* No assumption about data distribution (non-parametric).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* Easy to understand and interpret (visualizable).\n",
        "* Handles both **numerical and categorical data**.\n",
        "* No need for feature scaling or normalization.\n",
        "* Can handle **non-linear relationships**.\n",
        "* Works well with missing values (some implementations).\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* **Overfitting** (very deep trees).\n",
        "* Sensitive to small data changes (high variance).\n",
        "* Biased towards features with many levels (ID3).\n",
        "* Less accurate than ensemble methods (Random Forest, XGBoost).\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Finance:** Credit risk prediction (approve loan or not).\n",
        "* **Healthcare:** Disease diagnosis (symptom-based).\n",
        "* **Marketing:** Customer segmentation.\n",
        "* **E-commerce:** Recommending products based on user profile.\n",
        "* **Operations:** Predicting machine failure in manufacturing.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization – Decision Tree Workflow**\n",
        "\n",
        "```\n",
        "        Input Data\n",
        "            ↓\n",
        "     Choose Splitting Feature\n",
        "            ↓\n",
        "     Partition Data (Yes/No, >/<)\n",
        "            ↓\n",
        "   Create Child Nodes (subsets)\n",
        "            ↓\n",
        "  Repeat recursively until stop condition\n",
        "            ↓\n",
        "        Final Leaf Nodes\n",
        "            ↓\n",
        "     Prediction (Class/Value)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Variants of Decision Trees**\n",
        "\n",
        "* **ID3 (Iterative Dichotomiser 3):** Uses Information Gain (Entropy).\n",
        "* **C4.5:** Improvement over ID3, handles continuous variables.\n",
        "* **CART (Classification and Regression Trees):** Uses Gini (for classification) and MSE (for regression).\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Key Takeaways**\n",
        "\n",
        "* Decision Tree = interpretable, rule-based ML model.\n",
        "* Splits dataset recursively using **Entropy, Gini, or Variance reduction**.\n",
        "* Powerful but prone to **overfitting**, so usually combined into **Random Forest or Gradient Boosted Trees** for higher accuracy.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "B-kBXuD4P6u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Random Forest**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Random Forest** is an **ensemble learning method** that builds multiple **Decision Trees** and combines their results to improve accuracy and reduce overfitting.\n",
        "* Works for both **classification** and **regression** tasks.\n",
        "\n",
        "📌 **Key idea:**\n",
        "Instead of relying on one deep tree (which may overfit), Random Forest trains **many trees on random subsets of data and features**, then aggregates their predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "### **Step 1: Bootstrap Sampling (Bagging)**\n",
        "\n",
        "* From the training dataset of size $n$, draw **random samples with replacement** (bootstrap samples).\n",
        "* Each tree is trained on a different bootstrap sample.\n",
        "\n",
        "### **Step 2: Feature Randomness**\n",
        "\n",
        "* At each split in a tree, instead of considering **all features**, only a **random subset of features** is considered.\n",
        "* This decorrelates trees and improves generalization.\n",
        "\n",
        "### **Step 3: Train Multiple Decision Trees**\n",
        "\n",
        "* Build many decision trees (100s or 1000s).\n",
        "* Each tree learns slightly differently because of random data and random features.\n",
        "\n",
        "### **Step 4: Aggregate Predictions**\n",
        "\n",
        "* **Classification:** Majority voting (mode of all trees).\n",
        "* **Regression:** Average of predictions from all trees.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mathematical Formulation**\n",
        "\n",
        "For classification:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{mode}\\{ h_1(x), h_2(x), ..., h_T(x) \\}\n",
        "$$\n",
        "\n",
        "For regression:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} h_t(x)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $h_t(x)$ = prediction from tree $t$\n",
        "* $T$ = total number of trees\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Why Random Forest Works Better than a Single Tree?**\n",
        "\n",
        "* Decision Tree → low bias, high variance (overfits easily).\n",
        "* Random Forest → by averaging multiple trees:\n",
        "\n",
        "  * **Bias:** Slightly increased.\n",
        "  * **Variance:** Greatly reduced.\n",
        "  * **Result:** Better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* High accuracy and robustness.\n",
        "* Works well with both categorical and numerical features.\n",
        "* Handles missing values well.\n",
        "* Less overfitting compared to single decision tree.\n",
        "* Can measure **feature importance**.\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* Computationally expensive (many trees).\n",
        "* Less interpretable compared to a single tree.\n",
        "* Slower for real-time predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Feature Importance in Random Forest**\n",
        "\n",
        "* Feature importance is calculated using:\n",
        "\n",
        "  * **Gini Importance (Mean Decrease in Impurity)** → how much a feature reduces impurity across all trees.\n",
        "  * **Permutation Importance** → how much accuracy drops if a feature’s values are randomly shuffled.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Hyperparameters**\n",
        "\n",
        "* **n_estimators** → Number of trees (more trees = better but slower).\n",
        "* **max_depth** → Maximum depth of each tree.\n",
        "* **max_features** → Number of features to consider at each split.\n",
        "* **min_samples_split** → Minimum samples required to split a node.\n",
        "* **bootstrap** → Whether to sample with replacement.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Real-Life Applications**\n",
        "\n",
        "* **Finance:** Credit scoring, fraud detection.\n",
        "* **Healthcare:** Disease prediction (diabetes, cancer detection).\n",
        "* **E-commerce:** Recommendation engines, customer churn prediction.\n",
        "* **Agriculture:** Crop disease detection.\n",
        "* **Industry:** Predictive maintenance of machines.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Visualization – Random Forest Workflow**\n",
        "\n",
        "```\n",
        "            Training Data\n",
        "                  ↓\n",
        "        Bootstrap Sampling (Bagging)\n",
        "                  ↓\n",
        "     Build Decision Tree 1 (random features)\n",
        "     Build Decision Tree 2 (random features)\n",
        "     Build Decision Tree 3 (random features)\n",
        "                  ...\n",
        "           Build Decision Tree N\n",
        "                  ↓\n",
        "   Combine Predictions (Voting/Averaging)\n",
        "                  ↓\n",
        "           Final Prediction\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* Random Forest = **Bagging + Random Feature Selection**.\n",
        "* Improves accuracy and reduces overfitting compared to a single tree.\n",
        "* One of the best \"out-of-the-box\" ML algorithms.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "YkG0vr_3UTNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Support Vector Machine (SVM)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **SVM** is a **supervised learning algorithm** used for **classification** (mainly) and also **regression** (SVR).\n",
        "* It works by finding the **best decision boundary (hyperplane)** that separates classes with the **maximum margin**.\n",
        "* **Support Vectors** are the data points that are closest to the boundary and “support” its position.\n",
        "\n",
        "📌 Example:\n",
        "Imagine classifying emails as **Spam** or **Not Spam**. SVM will try to draw the **widest possible line (margin)** separating the two groups.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Core Idea: Maximum Margin Classifier**\n",
        "\n",
        "* Given two classes, SVM finds a **hyperplane** that best separates them.\n",
        "\n",
        "Equation of hyperplane:\n",
        "\n",
        "$$\n",
        "w \\cdot x + b = 0\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w$ = weight vector (perpendicular to hyperplane)\n",
        "* $b$ = bias (offset)\n",
        "\n",
        "Decision rule:\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "+1 & \\text{if } w \\cdot x + b \\geq 0 \\\\\n",
        "-1 & \\text{if } w \\cdot x + b < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "* **Margin:** Distance between the hyperplane and closest data points (support vectors).\n",
        "* SVM maximizes this margin → **better generalization**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Optimization Objective**\n",
        "\n",
        "SVM solves the following optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
        "$$\n",
        "\n",
        "Subject to constraints:\n",
        "\n",
        "$$\n",
        "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
        "$$\n",
        "\n",
        "Where $y_i \\in \\{+1, -1\\}$.\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* Classes are separated correctly.\n",
        "* Margin is maximized.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Handling Non-Separable Data (Soft Margin SVM)**\n",
        "\n",
        "* Real-world data is rarely perfectly separable.\n",
        "* Introduce **slack variables ($\\xi_i$)** to allow misclassification.\n",
        "\n",
        "Optimization:\n",
        "\n",
        "$$\n",
        "\\min_{w, b} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C$ = regularization parameter (tradeoff between margin and classification errors).\n",
        "* Large $C$ → less tolerance for misclassification (narrow margin).\n",
        "* Small $C$ → more tolerance (wider margin, more misclassifications).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Non-Linear Decision Boundaries (Kernel Trick)**\n",
        "\n",
        "* If data is **not linearly separable**, SVM uses the **Kernel Trick** to project data into a higher-dimensional space where it becomes separable.\n",
        "\n",
        "### Common Kernels:\n",
        "\n",
        "1. **Linear Kernel** → Best for linearly separable data.\n",
        "\n",
        "   $$\n",
        "   K(x, z) = x \\cdot z\n",
        "   $$\n",
        "\n",
        "2. **Polynomial Kernel**\n",
        "\n",
        "   $$\n",
        "   K(x, z) = (x \\cdot z + 1)^d\n",
        "   $$\n",
        "\n",
        "3. **RBF (Radial Basis Function / Gaussian) Kernel**\n",
        "\n",
        "   $$\n",
        "   K(x, z) = e^{-\\frac{\\|x-z\\|^2}{2\\sigma^2}}\n",
        "   $$\n",
        "\n",
        "   * Most commonly used for complex boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* Works well in **high-dimensional spaces**.\n",
        "* Effective with **non-linear boundaries** using kernels.\n",
        "* Robust against overfitting (with proper regularization).\n",
        "* Only depends on **support vectors**, not all data points.\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* Computationally expensive (not great for very large datasets).\n",
        "* Choosing the right **kernel & parameters** is tricky.\n",
        "* Less interpretable compared to logistic regression or decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Text Classification:** Spam detection, sentiment analysis.\n",
        "* **Bioinformatics:** Protein classification, cancer detection.\n",
        "* **Image Recognition:** Face recognition, handwriting classification.\n",
        "* **Finance:** Credit risk modeling, fraud detection.\n",
        "* **Healthcare:** Disease diagnosis (linear separability in medical tests).\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization – SVM Decision Boundary**\n",
        "\n",
        "```\n",
        "  Class +1  o o o       ||       x x x  Class -1\n",
        "  Class +1  o o o   <-- Margin -->   x x x  Class -1\n",
        "                ^    Hyperplane    ^\n",
        "                Support Vectors\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. SVM for Regression (SVR)**\n",
        "\n",
        "* Instead of classification, SVM can also be adapted for regression.\n",
        "* SVR tries to fit a function within a margin of tolerance (ε-insensitive loss).\n",
        "* Useful for stock price prediction, demand forecasting.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* SVM = **maximum margin classifier**.\n",
        "* Works for **linear & non-linear classification** (with kernels).\n",
        "* Great for **high-dimensional, small-to-medium datasets**.\n",
        "* Less suitable for very large datasets due to high computational cost.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "gXwajBdfZePG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. K-Nearest Neighbors (KNN)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **KNN** is a **non-parametric, instance-based, supervised ML algorithm**.\n",
        "* It can be used for both **classification** and **regression**.\n",
        "* Instead of building a model (like SVM or Decision Trees), KNN **makes predictions based on the closest data points (neighbors)** in the training dataset.\n",
        "\n",
        "📌 Example:\n",
        "Classify whether a fruit is an **Apple** or **Orange** by looking at the 5 most similar fruits (neighbors) based on weight and color.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "1. Choose the number of neighbors **K**.\n",
        "2. Compute the **distance** between the new point and all training points.\n",
        "3. Select the **K nearest neighbors**.\n",
        "4. **Classification:** Assign the most frequent class among neighbors.\n",
        "   **Regression:** Take the average of neighbors’ values.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Distance Metrics**\n",
        "\n",
        "KNN relies heavily on distance calculation. Common metrics:\n",
        "\n",
        "* **Euclidean Distance (default):**\n",
        "\n",
        "$$\n",
        "d(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\n",
        "$$\n",
        "\n",
        "* **Manhattan Distance:**\n",
        "\n",
        "$$\n",
        "d(x, y) = \\sum_{i=1}^n |x_i - y_i|\n",
        "$$\n",
        "\n",
        "* **Minkowski Distance (generalization):**\n",
        "\n",
        "$$\n",
        "d(x, y) = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p}\n",
        "$$\n",
        "\n",
        "* **Cosine Similarity (for text, high-dimensional data):**\n",
        "\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{x \\cdot y}{||x|| \\, ||y||}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Choosing K**\n",
        "\n",
        "* **Small K (e.g., K=1):**\n",
        "  → Model is highly sensitive to noise (overfitting).\n",
        "* **Large K:**\n",
        "  → Smoother decision boundary, but risk of underfitting.\n",
        "* **Rule of thumb:**\n",
        "  $K \\approx \\sqrt{n}$, where $n$ = number of samples.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Example (Classification)**\n",
        "\n",
        "Suppose we have a dataset of students’ study hours and pass/fail results.\n",
        "\n",
        "* New student studied **7 hours**.\n",
        "* Find K nearest students in training data.\n",
        "* If majority of them **passed**, predict \"Pass.\"\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* Very simple and intuitive.\n",
        "* No training phase (lazy learner).\n",
        "* Naturally handles **multi-class classification**.\n",
        "* Works well with small datasets.\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* Computationally expensive for large datasets (needs distance calculation for all points).\n",
        "* Sensitive to irrelevant/noisy features.\n",
        "* Requires **feature scaling** (since distance is sensitive to scale).\n",
        "* Choice of **K** and distance metric strongly affects performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Assumptions**\n",
        "\n",
        "* Nearby points are more likely to belong to the same class (local similarity assumption).\n",
        "* Distance metric correctly reflects similarity between points.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Real-Life Applications**\n",
        "\n",
        "* **Recommendation Systems:** Find similar users/items.\n",
        "* **Image Recognition:** Classify handwritten digits (MNIST dataset).\n",
        "* **Medical Diagnosis:** Classify patients based on symptoms.\n",
        "* **Finance:** Credit scoring (find customers similar to new applicant).\n",
        "* **Text Mining:** Document classification using cosine similarity.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Visualization – KNN Workflow**\n",
        "\n",
        "```\n",
        "          Input Data (Labeled dataset)\n",
        "                     ↓\n",
        "           Choose K & distance metric\n",
        "                     ↓\n",
        "   Compute distance from test sample to all training points\n",
        "                     ↓\n",
        "         Select K nearest neighbors\n",
        "                     ↓\n",
        "      Classification → Majority class wins\n",
        "      Regression     → Average of values\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* KNN = **\"You are who your neighbors are.\"**\n",
        "* Works by comparing similarity (distance) rather than learning explicit parameters.\n",
        "* Simple and interpretable, but computationally heavy for large datasets.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "VoxCcPElcdGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Naïve Bayes Classifier**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Naïve Bayes** is a **probabilistic supervised learning algorithm** based on **Bayes’ theorem** with a strong assumption:\n",
        "  **features are independent given the class label** (hence \"Naïve\").\n",
        "* Commonly used for **classification tasks** (not regression).\n",
        "* Extremely fast, interpretable, and efficient for **text classification**.\n",
        "\n",
        "📌 Example:\n",
        "Email classification (**Spam / Not Spam**) using words as features.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Bayes’ Theorem Refresher**\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(A|B)$: Posterior probability (probability of class $A$ given feature $B$).\n",
        "* $P(B|A)$: Likelihood (probability of feature $B$ given class $A$).\n",
        "* $P(A)$: Prior probability of class $A$.\n",
        "* $P(B)$: Evidence (probability of observing feature $B$).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Naïve Bayes Assumption**\n",
        "\n",
        "* All features are **conditionally independent** given the class label.\n",
        "* For multiple features $x_1, x_2, ..., x_n$:\n",
        "\n",
        "$$\n",
        "P(y|x_1, x_2, \\dots, x_n) \\propto P(y) \\prod_{i=1}^n P(x_i|y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Types of Naïve Bayes**\n",
        "\n",
        "Different variants depend on the type of features:\n",
        "\n",
        "### 🔹 **1. Gaussian Naïve Bayes**\n",
        "\n",
        "* Assumes features are continuous and follow a **normal distribution**.\n",
        "\n",
        "$$\n",
        "P(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y^2}} \\exp \\left( -\\frac{(x_i - \\mu_y)^2}{2 \\sigma_y^2} \\right)\n",
        "$$\n",
        "\n",
        "* **Use case:** Predicting diseases using medical measurements.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **2. Multinomial Naïve Bayes**\n",
        "\n",
        "* Features represent **counts or frequencies** (discrete values).\n",
        "* Often used for **text classification** (word counts).\n",
        "\n",
        "$$\n",
        "P(x_i | y) = \\frac{(n_{iy} + \\alpha)}{\\sum_j (n_{jy} + \\alpha)}\n",
        "$$\n",
        "\n",
        "(where $\\alpha$ = Laplace smoothing).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **3. Bernoulli Naïve Bayes**\n",
        "\n",
        "* Features are **binary (0 or 1)** (e.g., word present or not).\n",
        "* Good for **document classification** where absence/presence matters.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* Very fast and simple.\n",
        "* Works well with **high-dimensional data** (e.g., text).\n",
        "* Requires **less training data** compared to other classifiers.\n",
        "* Handles noisy data well with Laplace smoothing.\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* Assumption of **feature independence** is often unrealistic.\n",
        "* Poor estimates if probability is 0 (can be fixed with smoothing).\n",
        "* Not suitable for complex relationships between features.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Real-Life Applications**\n",
        "\n",
        "* **Spam Filtering** (emails).\n",
        "* **Sentiment Analysis** (positive/negative reviews).\n",
        "* **Document Categorization** (news, articles).\n",
        "* **Medical Diagnosis** (disease prediction).\n",
        "* **Recommendation Systems** (user preference classification).\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Example (Text Classification)**\n",
        "\n",
        "Task: Predict if a message is **Spam** or **Not Spam**.\n",
        "\n",
        "Training data:\n",
        "\n",
        "| Word    | Spam Count | Not Spam Count |\n",
        "| ------- | ---------- | -------------- |\n",
        "| \"Buy\"   | 3          | 1              |\n",
        "| \"Hello\" | 1          | 4              |\n",
        "| \"Offer\" | 4          | 1              |\n",
        "\n",
        "Message: **“Buy Offer Now”**\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Compute priors:\n",
        "\n",
        "   $$\n",
        "   P(Spam), \\, P(NotSpam)\n",
        "   $$\n",
        "2. Compute likelihood for each word given class.\n",
        "3. Multiply priors × likelihoods.\n",
        "4. Class with higher posterior = prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization**\n",
        "\n",
        "```\n",
        "   Input Features (Words, Symptoms, Attributes)\n",
        "                      ↓\n",
        "           Apply Bayes’ Theorem\n",
        "                      ↓\n",
        "      Calculate Posterior Probabilities\n",
        "                      ↓\n",
        "        Choose Class with Highest Probability\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Key Takeaways**\n",
        "\n",
        "* Naïve Bayes = **fast, probabilistic classifier**.\n",
        "* Strong independence assumption makes it “naïve.”\n",
        "* Best for **text classification, document filtering, sentiment analysis**.\n",
        "* Not great for data where features are highly correlated.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "XjMLT4MsgZW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Gradient Boosting & Its Variants (XGBoost, LightGBM, CatBoost)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Boosting** = An **ensemble technique** that builds models sequentially, where each new model focuses on correcting the errors made by previous ones.\n",
        "* **Gradient Boosting** = A boosting method where new models are trained to fit the **residual errors** (gradients of the loss function).\n",
        "* Unlike Bagging (Random Forest), which builds models in parallel, Boosting works **sequentially** and weights weak learners (usually decision trees).\n",
        "\n",
        "📌 Think of it as:\n",
        "\n",
        "1. Train a weak model (shallow tree).\n",
        "2. Compute errors (residuals).\n",
        "3. Train another model to fix those errors.\n",
        "4. Repeat until convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Gradient Boosting Workflow**\n",
        "\n",
        "1. Initialize model with a constant value (like mean of target).\n",
        "2. For each iteration:\n",
        "\n",
        "   * Compute the gradient of the loss function (residual errors).\n",
        "   * Fit a weak learner (decision tree) to these residuals.\n",
        "   * Update the model by adding this tree’s contribution.\n",
        "3. Final prediction is a weighted sum of all weak learners.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Key Equation**\n",
        "\n",
        "Final model:\n",
        "\n",
        "$$\n",
        "F_M(x) = F_{M-1}(x) + \\eta \\cdot h_M(x)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $F_M(x)$ = model after $M$ iterations\n",
        "* $h_M(x)$ = new weak learner (tree)\n",
        "* $\\eta$ = learning rate (controls contribution of each tree)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Pros & Cons**\n",
        "\n",
        "### ✅ Pros\n",
        "\n",
        "* Very high accuracy (often best-in-class).\n",
        "* Works with both regression & classification.\n",
        "* Handles complex non-linear relationships.\n",
        "* Flexible with loss functions (MSE, log-loss, etc.).\n",
        "\n",
        "### ❌ Cons\n",
        "\n",
        "* Computationally expensive (many sequential trees).\n",
        "* Prone to overfitting if not regularized.\n",
        "* Hyperparameter tuning is critical.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Real-Life Applications**\n",
        "\n",
        "* **Finance:** Credit scoring, fraud detection.\n",
        "* **Healthcare:** Disease risk prediction.\n",
        "* **Marketing:** Customer churn prediction.\n",
        "* **Search Engines:** Ranking results.\n",
        "* **Kaggle Competitions:** Almost always top solutions use XGBoost/LightGBM/CatBoost.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Popular Implementations**\n",
        "\n",
        "### 🔹 **XGBoost (Extreme Gradient Boosting)**\n",
        "\n",
        "* Highly optimized, scalable version of Gradient Boosting.\n",
        "* Features:\n",
        "\n",
        "  * Regularization (L1 & L2) to prevent overfitting.\n",
        "  * Parallel & distributed training support.\n",
        "  * Handles missing values automatically.\n",
        "  * Widely used in Kaggle competitions.\n",
        "* Best when: You need **accuracy + scalability** on medium-to-large structured/tabular datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **LightGBM (Light Gradient Boosting Machine)**\n",
        "\n",
        "* Developed by Microsoft, optimized for **speed & memory efficiency**.\n",
        "* Features:\n",
        "\n",
        "  * Uses **histogram-based splitting** (faster training).\n",
        "  * **Leaf-wise tree growth** (deeper, better accuracy but risk of overfitting).\n",
        "  * Handles **large datasets** with millions of rows.\n",
        "* Best when: Dataset is **very large (big data)** and speed is crucial.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **CatBoost**\n",
        "\n",
        "* Developed by Yandex, optimized for **categorical features**.\n",
        "* Features:\n",
        "\n",
        "  * Handles categorical variables **natively** (no need for one-hot encoding).\n",
        "  * Robust to overfitting with **ordered boosting**.\n",
        "  * Great default parameters → less tuning required.\n",
        "* Best when: Dataset has **many categorical features**.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Comparison Table**\n",
        "\n",
        "| Feature                  | XGBoost                | LightGBM                 | CatBoost               |\n",
        "| ------------------------ | ---------------------- | ------------------------ | ---------------------- |\n",
        "| **Speed**                | Fast                   | Very Fast (best)         | Moderate               |\n",
        "| **Memory Usage**         | Medium                 | Low                      | Medium                 |\n",
        "| **Categorical Handling** | Needs encoding         | Needs encoding           | Native support (best)  |\n",
        "| **Best For**             | Accuracy, flexible     | Large datasets           | Categorical-heavy data |\n",
        "| **Ease of Use**          | Medium (tuning needed) | Medium (risk of overfit) | Easy (good defaults)   |\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Visualization**\n",
        "\n",
        "```\n",
        "Data → Train Tree 1 → Compute Residuals → Train Tree 2 → ...\n",
        " → Train Tree 3 → ... → Final Ensemble Prediction\n",
        "```\n",
        "\n",
        "Boosting = \"Models learn sequentially from the mistakes of previous models.\"\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Key Hyperparameters**\n",
        "\n",
        "* **Learning rate (η):** Smaller → more accurate but slower.\n",
        "* **Number of trees (n_estimators):** More trees → higher accuracy but risk of overfitting.\n",
        "* **Max depth:** Limits complexity of trees.\n",
        "* **Subsample / colsample_bytree:** For regularization, prevents overfitting.\n",
        "* **Boosting type:** gbdt, dart, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* **Gradient Boosting** is one of the most accurate ML approaches for tabular data.\n",
        "* **XGBoost** = balanced (accuracy, scalability, robustness).\n",
        "* **LightGBM** = lightning fast, great for big datasets.\n",
        "* **CatBoost** = best for categorical-heavy datasets, easier tuning.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "JKDlQY2QoV7r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ren8hkKGopAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}