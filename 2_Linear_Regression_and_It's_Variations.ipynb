{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Linear Regression**\n",
        "---\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Definition:**\n",
        "  Linear Regression is a **supervised learning algorithm** used for **predicting continuous values** based on one or more input features.\n",
        "* **Idea:** Fit a straight line (or hyperplane in higher dimensions) that best describes the relationship between inputs (**X**) and output (**Y**).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "* We assume a **linear relationship** between dependent variable (Y) and independent variables (X).\n",
        "* General form:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = dependent variable (target)\n",
        "* $X_i$ = independent variables (features)\n",
        "* $\\beta_i$ = coefficients (weights)\n",
        "* $\\beta_0$ = intercept (bias term)\n",
        "* $\\epsilon$ = error term (difference between prediction & actual)\n",
        "\n",
        "üìä **Example:**\n",
        "Predicting house price (Y) based on size (X).\n",
        "Equation:\n",
        "\n",
        "$$\n",
        "Price = \\beta_0 + \\beta_1 \\times Size\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mathematical Intuition**\n",
        "\n",
        "* Goal: Find coefficients ($\\beta$) that minimize the difference between **predicted values** and **actual values**.\n",
        "* Cost function (Mean Squared Error, MSE):\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$ = actual value\n",
        "\n",
        "* $\\hat{y}_i$ = predicted value ($\\beta_0 + \\beta_1 X_i$)\n",
        "\n",
        "* $n$ = number of data points\n",
        "\n",
        "* Optimization is done using:\n",
        "\n",
        "  * **Analytical method (Normal Equation):**\n",
        "\n",
        "  $$\n",
        "  \\beta = (X^T X)^{-1} X^T y\n",
        "  $$\n",
        "\n",
        "  * **Iterative method (Gradient Descent):**\n",
        "    Update rule:\n",
        "\n",
        "  $$\n",
        "  \\beta_j = \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}\n",
        "  $$\n",
        "\n",
        "  where $\\alpha$ is learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Assumptions of Linear Regression**\n",
        "\n",
        "Linear Regression works best when these assumptions hold:\n",
        "\n",
        "1. **Linearity:** Relationship between features and target is linear.\n",
        "2. **Independence:** Observations are independent of each other.\n",
        "3. **Homoscedasticity:** Constant variance of errors across values of independent variables.\n",
        "4. **Normality:** Errors (residuals) are normally distributed.\n",
        "5. **No multicollinearity:** Independent variables should not be highly correlated.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* Simple and easy to understand.\n",
        "* Fast to train and interpret.\n",
        "* Works well for linearly related data.\n",
        "* Can be extended to multiple features (Multiple Linear Regression).\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Assumes linear relationship (fails with non-linear data).\n",
        "* Sensitive to outliers (a single extreme value can distort results).\n",
        "* Requires assumptions (linearity, normality, homoscedasticity).\n",
        "* Poor performance if features are highly correlated (multicollinearity).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Variants of Linear Regression**\n",
        "\n",
        "* **Simple Linear Regression** ‚Üí One independent variable.\n",
        "* **Multiple Linear Regression** ‚Üí Multiple independent variables.\n",
        "* **Regularized Regression:**\n",
        "\n",
        "  * **Ridge Regression (L2)** ‚Üí adds penalty $\\lambda \\sum \\beta^2$.\n",
        "  * **Lasso Regression (L1)** ‚Üí adds penalty $\\lambda \\sum |\\beta|$, useful for feature selection.\n",
        "  * **Elastic Net** ‚Üí combination of L1 and L2.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Economics:** Predicting GDP growth based on interest rates, inflation.\n",
        "* **Business:** Forecasting sales from marketing spend.\n",
        "* **Healthcare:** Predicting patient‚Äôs blood pressure based on age, weight.\n",
        "* **Real Estate:** Estimating house prices.\n",
        "* **Sports:** Predicting player‚Äôs performance from fitness metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Flowchart ‚Äì Linear Regression Workflow**\n",
        "\n",
        "```\n",
        "          Dataset (X, Y)\n",
        "                |\n",
        "          Data Preprocessing\n",
        "                |\n",
        "         Train Model (fit line)\n",
        "                |\n",
        "        Calculate Error (MSE)\n",
        "                |\n",
        "       Optimize Coefficients Œ≤\n",
        "                |\n",
        "         Best Fit Line/Plane\n",
        "                |\n",
        "         Make Predictions ≈∂\n",
        "```\n",
        "---\n",
        "\n",
        "‚úÖ **Key Takeaways:**\n",
        "\n",
        "* Linear Regression = simplest ML algorithm for predicting continuous values.\n",
        "* Works by minimizing error (MSE) between predicted and actual values.\n",
        "* Easy to interpret but limited to linear relationships.\n",
        "* Extensions (Ridge, Lasso) handle multicollinearity & overfitting.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **A. Simple Linear Regression (SLR)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Definition:**\n",
        "  Simple Linear Regression is a **statistical method** to predict the value of a **dependent variable (Y)** using **one independent variable (X)**.\n",
        "* Relationship is modeled with a straight line.\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = target (dependent variable)\n",
        "* $X$ = predictor (independent variable)\n",
        "* $\\beta_0$ = intercept (value of Y when X=0)\n",
        "* $\\beta_1$ = slope (change in Y for 1 unit change in X)\n",
        "* $\\epsilon$ = error term\n",
        "\n",
        "üìä **Example:** Predicting a student‚Äôs exam score (Y) based on hours studied (X).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "1. Collect data points: $(X_1, Y_1), (X_2, Y_2), ‚Ä¶ (X_n, Y_n)$.\n",
        "2. Fit a **line of best fit** through data.\n",
        "3. Prediction: For a new input $X_{new}$, output is:\n",
        "\n",
        "$$\n",
        "\\hat{Y} = \\beta_0 + \\beta_1 X_{new}\n",
        "$$\n",
        "\n",
        "4. Line of best fit is chosen by minimizing the **sum of squared errors** (Least Squares Method).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mathematical Intuition**\n",
        "\n",
        "* **Goal:** Minimize error between actual and predicted values.\n",
        "* Error (residual):\n",
        "\n",
        "$$\n",
        "e_i = y_i - \\hat{y}_i\n",
        "$$\n",
        "\n",
        "* Cost function (Mean Squared Error):\n",
        "\n",
        "$$\n",
        "J(\\beta_0, \\beta_1) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2\n",
        "$$\n",
        "\n",
        "* Optimal slope ($\\beta_1$) and intercept ($\\beta_0$):\n",
        "\n",
        "$$\n",
        "\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Assumptions of SLR**\n",
        "\n",
        "1. Linear relationship exists between X and Y.\n",
        "2. Errors (residuals) are normally distributed.\n",
        "3. Homoscedasticity (equal variance of residuals).\n",
        "4. Independence of observations.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* Easy to understand and implement.\n",
        "* Computationally efficient.\n",
        "* Provides interpretable coefficients ($\\beta_0, \\beta_1$).\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Only works well for **linear relationships**.\n",
        "* Sensitive to **outliers**.\n",
        "* Cannot handle **multiple features** (that‚Äôs Multiple Linear Regression).\n",
        "* Strong assumptions (normality, homoscedasticity).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Real-Life Applications**\n",
        "\n",
        "* Predicting weight from height.\n",
        "* Predicting sales from advertising spend.\n",
        "* Estimating temperature from altitude.\n",
        "* Relationship between study hours and exam scores.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Key Takeaways:**\n",
        "\n",
        "* **SLR = one feature ‚Üí one target.**\n",
        "* Fits a straight line by minimizing squared errors.\n",
        "* Very interpretable but limited in real-world use since most problems need multiple predictors.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **B. Multiple Linear Regression (MLR)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "* **Definition:**\n",
        "  Multiple Linear Regression is a statistical technique to model the relationship between a **dependent variable (Y)** and **two or more independent variables (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çô)**.\n",
        "\n",
        "* **Equation:**\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = dependent variable (target)\n",
        "* $X_1, X_2, ‚Ä¶, X_n$ = independent variables (features)\n",
        "* $\\beta_0$ = intercept (bias)\n",
        "* $\\beta_i$ = coefficients (weights for features)\n",
        "* $\\epsilon$ = error term\n",
        "\n",
        "üìä **Example:** Predicting house price (Y) using features like size (X‚ÇÅ), number of rooms (X‚ÇÇ), and location score (X‚ÇÉ).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Working Principle**\n",
        "\n",
        "* Collect dataset with multiple predictors.\n",
        "* Fit a **hyperplane** (not just a line) in n-dimensional space.\n",
        "* Predictions are made by plugging in feature values into the regression equation.\n",
        "* Coefficients ($\\beta$) are chosen to minimize the **sum of squared errors**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mathematical Intuition**\n",
        "\n",
        "### **Matrix Form Representation**\n",
        "\n",
        "For n features:\n",
        "\n",
        "$$\n",
        "Y = X\\beta + \\epsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Y$ = vector of actual outputs (m √ó 1)\n",
        "* $X$ = feature matrix (m √ó n), with a column of ones for intercept\n",
        "* $\\beta$ = vector of coefficients (n √ó 1)\n",
        "* $\\epsilon$ = error vector\n",
        "\n",
        "### **Cost Function (Mean Squared Error, MSE):**\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "### **Solution (Normal Equation):**\n",
        "\n",
        "$$\n",
        "\\hat{\\beta} = (X^T X)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "Or, when dataset is large ‚Üí use **Gradient Descent** to iteratively update coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Assumptions of MLR**\n",
        "\n",
        "1. **Linearity:** Relationship between predictors and target is linear.\n",
        "2. **Independence:** Observations are independent.\n",
        "3. **Homoscedasticity:** Constant variance of residuals.\n",
        "4. **Normality:** Residuals are normally distributed.\n",
        "5. **No Multicollinearity:** Predictors should not be highly correlated.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* Handles multiple features ‚Üí better predictive power.\n",
        "* Coefficients show feature importance.\n",
        "* Easy to implement & interpret (compared to complex ML).\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* Assumes linear relationships.\n",
        "* Sensitive to **outliers**.\n",
        "* **Multicollinearity** (highly correlated features) can make coefficients unreliable.\n",
        "* Overfitting possible with too many predictors.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Real-Life Applications**\n",
        "\n",
        "* **Real Estate:** Predict house prices from size, rooms, location, amenities.\n",
        "* **Marketing:** Predict sales from ad spend, promotions, and discounts.\n",
        "* **Healthcare:** Predict disease risk from age, lifestyle, and medical test results.\n",
        "* **Economics:** Predict GDP from investment, population, and exports.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Visualization ‚Äì From Line to Hyperplane**\n",
        "\n",
        "* **SLR:** Fits a **line** in 2D (X vs Y).\n",
        "* **MLR with 2 features:** Fits a **plane** in 3D (X‚ÇÅ, X‚ÇÇ vs Y).\n",
        "* **MLR with n features:** Fits a **hyperplane** in n+1 dimensional space.\n",
        "---\n",
        "\n",
        "‚úÖ **Key Takeaways:**\n",
        "\n",
        "* **MLR extends SLR** by allowing multiple features.\n",
        "* Coefficients ($\\beta$) show effect of each feature on the target.\n",
        "* Works well for linearly dependent problems but suffers with multicollinearity.\n",
        "* Foundation for **advanced regression techniques** (Ridge, Lasso, Elastic Net).\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **C. Advanced Regression Techniques**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Why Do We Need Regularization?**\n",
        "\n",
        "* In **Multiple Linear Regression**, when we have:\n",
        "\n",
        "  * **Too many features** ‚Üí risk of **overfitting**.\n",
        "  * **Highly correlated features** (multicollinearity) ‚Üí unstable coefficients.\n",
        "\n",
        "* **Regularization** solves this by adding a **penalty term** to the cost function, shrinking coefficients, and reducing variance.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "### **Idea**\n",
        "\n",
        "* Adds **L2 penalty (squared magnitude of coefficients)** to cost function.\n",
        "* Shrinks coefficients toward zero but **never makes them exactly zero**.\n",
        "\n",
        "### **Mathematical Form**\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* First term = usual MSE\n",
        "* Second term = penalty (L2 norm)\n",
        "* $\\lambda$ = regularization parameter (controls penalty strength)\n",
        "\n",
        "### **Effect**\n",
        "\n",
        "* Large coefficients are penalized ‚Üí helps control multicollinearity.\n",
        "* Keeps all features, but shrinks their influence.\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "* Handles multicollinearity well.\n",
        "* Reduces model complexity (prevents overfitting).\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "* Does **not perform feature selection** (all variables kept).\n",
        "\n",
        "### **Use Case**\n",
        "\n",
        "* Predicting house prices with many correlated features (e.g., area, no. of rooms, total sqft).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "### **Idea**\n",
        "\n",
        "* Adds **L1 penalty (absolute value of coefficients)**.\n",
        "* Can shrink some coefficients to **exactly zero** ‚Üí acts as **feature selection**.\n",
        "\n",
        "### **Mathematical Form**\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
        "$$\n",
        "\n",
        "### **Effect**\n",
        "\n",
        "* Some coefficients become exactly zero ‚Üí irrelevant features removed.\n",
        "* Produces a **sparse model**.\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "* Performs automatic **feature selection**.\n",
        "* Good when we suspect many features are irrelevant.\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "* Can behave inconsistently when features are highly correlated (randomly selects one feature).\n",
        "\n",
        "### **Use Case**\n",
        "\n",
        "* Text classification (Bag-of-Words with thousands of features, but only few matter).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Elastic Net (L1 + L2 Regularization)**\n",
        "\n",
        "### **Idea**\n",
        "\n",
        "* Combines Ridge (L2) + Lasso (L1) penalties.\n",
        "* Good balance between **shrinkage** and **feature selection**.\n",
        "\n",
        "### **Mathematical Form**\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
        "$$\n",
        "\n",
        "Or often written as:\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\lambda [ \\alpha \\sum |\\beta_j| + (1-\\alpha) \\sum \\beta_j^2 ]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\alpha = 1$ ‚Üí Lasso,\n",
        "* $\\alpha = 0$ ‚Üí Ridge.\n",
        "\n",
        "### **Effect**\n",
        "\n",
        "* Handles correlated features better than Lasso.\n",
        "* Performs variable selection + coefficient shrinkage.\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "* Flexible (adjust Œ± to control balance).\n",
        "* Useful when features are highly correlated and some irrelevant.\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "* More complex (two hyperparameters: Œª, Œ±).\n",
        "\n",
        "### **Use Case**\n",
        "\n",
        "* Genomic data (many correlated features, only a few important).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Visual Intuition**\n",
        "\n",
        "* **Ridge (L2):** Penalizes large coefficients, shrinks them smoothly (circle constraint).\n",
        "* **Lasso (L1):** Shrinks coefficients aggressively ‚Üí some become exactly 0 (diamond constraint).\n",
        "* **Elastic Net:** Mixture of both (ellipse/diamond hybrid constraint).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Pros & Cons Summary**\n",
        "\n",
        "| Technique      | Penalty Type | Coefficients Become Zero? | Handles Multicollinearity | Feature Selection | Best Use Case                          |\n",
        "| -------------- | ------------ | ------------------------- | ------------------------- | ----------------- | -------------------------------------- |\n",
        "| **Ridge**      | L2           | ‚ùå No                      | ‚úÖ Yes                     | ‚ùå No              | Many correlated features               |\n",
        "| **Lasso**      | L1           | ‚úÖ Yes                     | ‚ùå Not well                | ‚úÖ Yes             | Feature selection, sparse data         |\n",
        "| **ElasticNet** | L1 + L2      | ‚úÖ Yes (some)              | ‚úÖ Yes                     | ‚úÖ Yes             | High-dimensional & correlated features |\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Real-Life Applications**\n",
        "\n",
        "* **Ridge:** Predicting real estate prices with many related variables.\n",
        "* **Lasso:** Selecting important words in NLP text classification.\n",
        "* **Elastic Net:** Genomic research (thousands of gene features, some correlated).\n",
        "\n",
        "---\n",
        "\n",
        "üìä **Flowchart ‚Äì Choosing the Right Regression**\n",
        "\n",
        "```\n",
        "           Too many features?\n",
        "                 |\n",
        "         +-------+--------+\n",
        "         |                |\n",
        "      Yes (dimensionality) No\n",
        "         |                |\n",
        "     Lasso/Elastic Net    Ridge\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Key Takeaways:**\n",
        "\n",
        "* **Ridge (L2):** Shrinks coefficients but keeps all features.\n",
        "* **Lasso (L1):** Shrinks some coefficients to zero ‚Üí feature selection.\n",
        "* **Elastic Net:** Mix of both ‚Üí good for correlated features + feature selection.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "MhY_z7bHn3ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk9NJKQol4Zd"
      },
      "outputs": [],
      "source": []
    }
  ]
}