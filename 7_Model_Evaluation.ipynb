{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Model Evaluation Metrics**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Confusion Matrix (The Foundation)**\n",
        "\n",
        "For a **binary classification problem** (Positive vs Negative), we classify predictions into four categories:\n",
        "\n",
        "|                     | **Predicted Positive** | **Predicted Negative** |\n",
        "| ------------------- | ---------------------- | ---------------------- |\n",
        "| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
        "| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Accuracy**\n",
        "\n",
        "* The ratio of correctly predicted samples to total samples.\n",
        "  [\n",
        "  Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "  ]\n",
        "\n",
        "‚úÖ Good when: Classes are balanced.\n",
        "‚ùå Misleading when: Dataset is **imbalanced** (e.g., fraud detection with 99% normal cases).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Precision (Positive Predictive Value)**\n",
        "\n",
        "* Of all predicted positives, how many are truly positive?\n",
        "  [\n",
        "  Precision = \\frac{TP}{TP + FP}\n",
        "  ]\n",
        "\n",
        "‚úÖ Important when **false positives are costly**.\n",
        "üìå Example: Spam detection (you don‚Äôt want to mark genuine emails as spam).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Recall (Sensitivity / True Positive Rate)**\n",
        "\n",
        "* Of all actual positives, how many did we correctly predict?\n",
        "  [\n",
        "  Recall = \\frac{TP}{TP + FN}\n",
        "  ]\n",
        "\n",
        "‚úÖ Important when **false negatives are costly**.\n",
        "üìå Example: Cancer detection (you don‚Äôt want to miss a real cancer case).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. F1-Score (Harmonic Mean of Precision & Recall)**\n",
        "\n",
        "* Balances precision and recall.\n",
        "  [\n",
        "  F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
        "  ]\n",
        "\n",
        "‚úÖ Good when you need a balance between **precision and recall**.\n",
        "üìå Example: Information retrieval, fraud detection.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. ROC Curve (Receiver Operating Characteristic)**\n",
        "\n",
        "* Plots **True Positive Rate (Recall)** vs **False Positive Rate (FPR = FP / (FP+TN))** at different thresholds.\n",
        "* A good classifier ‚Üí curve near top-left corner.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. AUC (Area Under ROC Curve)**\n",
        "\n",
        "* Measures classifier‚Äôs ability to distinguish between classes.\n",
        "* Ranges from 0.5 (random guessing) to 1.0 (perfect classifier).\n",
        "\n",
        "‚úÖ Higher AUC = Better model.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. When to Use Which?**\n",
        "\n",
        "| Metric        | Best Used When                                                       |\n",
        "| ------------- | -------------------------------------------------------------------- |\n",
        "| **Accuracy**  | Balanced classes, general correctness                                |\n",
        "| **Precision** | Cost of false positives is high (spam filters, recommendations)      |\n",
        "| **Recall**    | Cost of false negatives is high (medical diagnosis, fraud detection) |\n",
        "| **F1-Score**  | Need balance between precision & recall (imbalanced data)            |\n",
        "| **ROC-AUC**   | Comparing models‚Äô ability to separate classes (ranking performance)  |\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Visualization (Conceptual)**\n",
        "\n",
        "üìå Confusion Matrix Example (Cancer Detection):\n",
        "\n",
        "```\n",
        "                Predicted\n",
        "             |  Positive   Negative\n",
        "Actual   +   |    80         20\n",
        "        -   |    10        890\n",
        "```\n",
        "\n",
        "* Precision = 80 / (80+10) = 0.89\n",
        "* Recall = 80 / (80+20) = 0.80\n",
        "* F1 = 0.84\n",
        "* Accuracy = (80+890)/1000 = 0.97\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* Accuracy is not always enough (especially with imbalanced data).\n",
        "* Precision vs Recall depends on whether you care more about **false positives** or **false negatives**.\n",
        "* F1-score = balance.\n",
        "* ROC-AUC = ranking ability across thresholds.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# **2. Bias‚ÄìVariance Tradeoff**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Sources of Error in ML Models**\n",
        "\n",
        "When a model predicts, the total error can be decomposed into three parts:\n",
        "\n",
        "[\n",
        "Total Error = Bias^2 + Variance + Irreducible Error\n",
        "]\n",
        "\n",
        "* **Bias:** Error from overly simple assumptions in the model.\n",
        "* **Variance:** Error from sensitivity to training data (too complex model).\n",
        "* **Irreducible Error (Noise):** Error due to randomness in data (cannot be fixed).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. What is Bias?**\n",
        "\n",
        "* **Bias = Difference between model‚Äôs average prediction and the true value.**\n",
        "* High bias ‚Üí model is too simple, underfitting.\n",
        "* Example: Linear regression trying to fit non-linear data ‚Üí biased predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. What is Variance?**\n",
        "\n",
        "* **Variance = How much model predictions change if trained on different datasets.**\n",
        "* High variance ‚Üí model is too complex, overfitting.\n",
        "* Example: A deep decision tree memorizes training data but performs poorly on test data.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Tradeoff**\n",
        "\n",
        "* If you **increase complexity** ‚Üí bias ‚Üì but variance ‚Üë.\n",
        "* If you **decrease complexity** ‚Üí variance ‚Üì but bias ‚Üë.\n",
        "* Goal: Find the **sweet spot** where total error is minimized.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Visualization (Conceptual)**\n",
        "\n",
        "üéØ **Bias‚ÄìVariance in terms of shooting arrows at a target:**\n",
        "\n",
        "* **High Bias, Low Variance:** Shots are grouped but far from the bullseye (systematically wrong ‚Üí underfit).\n",
        "* **Low Bias, High Variance:** Shots are scattered around bullseye (inconsistent ‚Üí overfit).\n",
        "* **Low Bias, Low Variance:** Shots are tightly around the bullseye (ideal).\n",
        "* **High Bias, High Variance:** Shots are scattered far away (worst).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Graph of Error vs Model Complexity**\n",
        "\n",
        "```\n",
        "Error\n",
        "‚îÇ        Bias Error  \\\n",
        "‚îÇ                     \\\n",
        "‚îÇ                      \\\n",
        "‚îÇ                        \\__ Total Error\n",
        "‚îÇ          Variance Error /\n",
        "‚îÇ         /              /\n",
        "‚îÇ        /              /\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Model Complexity\n",
        "```\n",
        "\n",
        "* At low complexity ‚Üí high bias, low variance.\n",
        "* At high complexity ‚Üí low bias, high variance.\n",
        "* Optimal complexity = minimum total error.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Strategies to Manage Tradeoff**\n",
        "\n",
        "* **Reduce Bias (Underfitting):**\n",
        "\n",
        "  * Add more features.\n",
        "  * Use more complex models.\n",
        "  * Reduce regularization (lower Œª in Ridge/Lasso).\n",
        "\n",
        "* **Reduce Variance (Overfitting):**\n",
        "\n",
        "  * Use simpler models.\n",
        "  * Regularization (Ridge, Lasso, Dropout in NN).\n",
        "  * Collect more training data.\n",
        "  * Ensemble methods (Bagging, Random Forest).\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Real-Life Examples**\n",
        "\n",
        "* **High Bias (Underfitting):** Predicting house prices using only number of bedrooms (ignores location, size, etc.).\n",
        "* **High Variance (Overfitting):** Memorizing exact past house sales ‚Üí perfect training accuracy but poor test accuracy.\n",
        "* **Balanced Model:** Captures main patterns without memorizing noise.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Key Takeaways**\n",
        "\n",
        "* **Bias = Wrong assumptions ‚Üí underfitting.**\n",
        "* **Variance = Over-sensitivity ‚Üí overfitting.**\n",
        "* Tradeoff = Finding optimal model complexity.\n",
        "* Goal: **Low bias + Low variance** (but not zero).\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **3. Cross-Validation**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Why Do We Need Cross-Validation?**\n",
        "\n",
        "* Training accuracy alone doesn‚Äôt tell us if the model generalizes.\n",
        "* A single **train-test split** might give misleading results (depending on the split).\n",
        "* Cross-validation ensures the model is tested on **different subsets** of data, reducing variance in evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Basic Idea**\n",
        "\n",
        "* Split data into multiple folds (subsets).\n",
        "* Train model on **k-1 folds** and test on the remaining fold.\n",
        "* Repeat until each fold has been used as test once.\n",
        "* Average results across folds.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Types of Cross-Validation**\n",
        "\n",
        "### üîπ **Hold-Out Method (Train-Test Split)**\n",
        "\n",
        "* Simplest: Split dataset (e.g., 80% train, 20% test).\n",
        "* Fast but can give **biased/unstable results** depending on split.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **k-Fold Cross-Validation**\n",
        "\n",
        "* Divide data into **k equal folds**.\n",
        "* Train on k-1 folds, test on 1 fold.\n",
        "* Repeat k times.\n",
        "* Average metrics ‚Üí more stable performance estimate.\n",
        "\n",
        "üìå Common choice: **k = 5 or 10**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Stratified k-Fold**\n",
        "\n",
        "* Like k-Fold, but ensures class proportions are preserved in each fold.\n",
        "* Important for **imbalanced datasets** (e.g., fraud detection).\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Leave-One-Out Cross-Validation (LOOCV)**\n",
        "\n",
        "* Extreme case of k-Fold where k = n (number of samples).\n",
        "* Train on n-1 samples, test on 1 sample.\n",
        "* Very accurate but computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Leave-P-Out Cross-Validation**\n",
        "\n",
        "* Leave p samples out for testing, use rest for training.\n",
        "* Generalization of LOOCV (p=1).\n",
        "* Too costly for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Nested Cross-Validation**\n",
        "\n",
        "* Used for **model selection + evaluation** simultaneously.\n",
        "* Outer loop = evaluates model.\n",
        "* Inner loop = tunes hyperparameters (Grid Search, Random Search, Bayesian Optimization).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Workflow Example (5-Fold CV)**\n",
        "\n",
        "Dataset = 100 samples, k=5.\n",
        "\n",
        "* Fold 1: Train (80), Test (20)\n",
        "* Fold 2: Train (80), Test (20)\n",
        "* Fold 3: Train (80), Test (20)\n",
        "* Fold 4: Train (80), Test (20)\n",
        "* Fold 5: Train (80), Test (20)\n",
        "\n",
        "Final performance = average of all 5 test results.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Pros & Cons**\n",
        "\n",
        "### ‚úÖ Pros\n",
        "\n",
        "* More reliable than single train-test split.\n",
        "* Reduces risk of **overfitting/underfitting evaluation**.\n",
        "* Uses data efficiently (all points used for training & testing).\n",
        "\n",
        "### ‚ùå Cons\n",
        "\n",
        "* More computationally expensive (especially LOOCV).\n",
        "* Not always suitable for **time series** (since order matters).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Special Case: Cross-Validation in Time Series**\n",
        "\n",
        "* Cannot randomly shuffle (future depends on past).\n",
        "* Use **rolling/expanding window CV**:\n",
        "\n",
        "  * Train on first 70%, test on next 10%, move window forward.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Key Takeaways**\n",
        "\n",
        "* Cross-validation provides a **robust estimate of model performance**.\n",
        "* **k-Fold (k=5 or 10)** is the most commonly used.\n",
        "* Use **Stratified CV** for imbalanced datasets.\n",
        "* Use **Nested CV** when tuning hyperparameters.\n",
        "* Use **Time Series CV** for sequential data.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# **4. Regularization Techniques in Machine Learning**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Why Do We Need Regularization?**\n",
        "\n",
        "* In high-dimensional data (lots of features), models can overfit.\n",
        "* Overfitting = memorizing noise instead of learning patterns.\n",
        "* Regularization helps by **adding a penalty term** to the loss function ‚Üí discourages overly complex models.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. General Form**\n",
        "\n",
        "For regression, the typical cost function is **Mean Squared Error (MSE):**\n",
        "\n",
        "[\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "]\n",
        "\n",
        "Regularization adds a **penalty**:\n",
        "\n",
        "[\n",
        "J(\\theta) = \\text{Loss Function} + \\lambda \\cdot \\text{Penalty}\n",
        "]\n",
        "\n",
        "* **Œª (lambda)** = regularization strength.\n",
        "\n",
        "  * Large Œª ‚Üí stronger penalty ‚Üí simpler model.\n",
        "  * Small Œª ‚Üí weaker penalty ‚Üí closer to normal regression.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* Adds penalty = sum of squared coefficients.\n",
        "\n",
        "[\n",
        "J(\\theta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "]\n",
        "\n",
        "* Shrinks coefficients towards zero, but **never exactly zero**.\n",
        "* Works well when: Many small/medium correlated features.\n",
        "\n",
        "‚úÖ Pros: Prevents overfitting, stable solution.\n",
        "‚ùå Cons: Doesn‚Äôt perform feature selection (all features remain).\n",
        "\n",
        "üìå **Use Case:** Ridge is good when you believe **all features contribute** but just want to shrink their impact.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* Adds penalty = sum of absolute values of coefficients.\n",
        "\n",
        "[\n",
        "J(\\theta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "]\n",
        "\n",
        "* Can shrink some coefficients **exactly to zero** ‚Üí feature selection.\n",
        "* Works well when: Only a few features are important.\n",
        "\n",
        "‚úÖ Pros: Performs automatic **feature selection**.\n",
        "‚ùå Cons: Can be unstable if features are highly correlated.\n",
        "\n",
        "üìå **Use Case:** Lasso is good when you expect **sparsity** (only some features matter).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Elastic Net (Combination of L1 & L2)**\n",
        "\n",
        "* Hybrid of Ridge + Lasso.\n",
        "\n",
        "[\n",
        "J(\\theta) = \\text{MSE} + \\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2\n",
        "]\n",
        "\n",
        "* Balances **feature selection (L1)** and **stability (L2)**.\n",
        "* Good when: Features are highly correlated, or you need both benefits.\n",
        "\n",
        "‚úÖ Pros: Flexible, handles correlated features better.\n",
        "‚ùå Cons: Two hyperparameters to tune.\n",
        "\n",
        "üìå **Use Case:** Best for **high-dimensional datasets** (like genomics, text classification).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Visualization (Effect on Coefficients)**\n",
        "\n",
        "* **No Regularization:** Coefficients can grow very large.\n",
        "* **Ridge:** Coefficients shrink, but none go to zero.\n",
        "* **Lasso:** Some coefficients = 0 (feature elimination).\n",
        "* **Elastic Net:** Combination ‚Äî some shrink, some eliminated.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Beyond Linear Models (Regularization in Other Models)**\n",
        "\n",
        "* **Decision Trees / Ensembles**\n",
        "\n",
        "  * Control depth, min samples, pruning (prevents overfitting).\n",
        "* **Neural Networks**\n",
        "\n",
        "  * L1/L2 penalties on weights.\n",
        "  * **Dropout**: Randomly remove neurons during training.\n",
        "  * **Early Stopping**: Stop training when validation error increases.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Choosing Œª (Regularization Strength)**\n",
        "\n",
        "* If Œª = 0 ‚Üí no regularization (risk of overfitting).\n",
        "* If Œª ‚Üí ‚àû ‚Üí all coefficients shrink to 0 (underfitting).\n",
        "* Use **Cross-Validation (CV)** to find optimal Œª.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Real-Life Applications**\n",
        "\n",
        "* **Finance:** Prevents overfitting in stock price prediction with many indicators.\n",
        "* **Genomics:** Lasso selects important genes for disease prediction.\n",
        "* **Text Classification:** Elastic Net handles thousands of word features (TF-IDF).\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Key Takeaways**\n",
        "\n",
        "* **Ridge (L2):** Shrinks coefficients, keeps all features.\n",
        "* **Lasso (L1):** Eliminates irrelevant features.\n",
        "* **Elastic Net (L1+L2):** Best of both worlds, especially for correlated features.\n",
        "* Regularization = crucial for reducing **variance** while controlling **bias**.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "dirSQG7fhloq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqc18TKrbTw8"
      },
      "outputs": [],
      "source": []
    }
  ]
}